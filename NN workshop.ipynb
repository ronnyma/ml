{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Velkommen til workshop i nevrale nettverk hos Husbanken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det første vi må gjøre er å importere bibliotekene vi trenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist #Dette er datasettet - vi slipper å laste opp, convertere erc.\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#Importer lag og modell fra Keras\n",
    "from keras.layers import Dense \n",
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hjelpefunksjoner for å nullstille grafen, visualisere litt, prediksjon og utskrift av test-data.\n",
    "from keras import backend as K\n",
    "\n",
    "def init_graph():\n",
    "    K.clear_session()\n",
    "    \n",
    "def plot_graph(history, loss, accuracy):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n",
    "    \n",
    "def predictDigit(n):\n",
    "    print(\"Algoritmen gjettet \", np.argmax(model.predict(X_test[n:n+1])))\n",
    "    \n",
    "def printDigit(i):\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(x_test[i], cmap='gray', interpolation='none')\n",
    "    print(\"Riktig svar er: \", y_test[i])\n",
    "    \n",
    "def call_eval(p):\n",
    "    predictDigit(p)\n",
    "    printDigit(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Det er vanlig å dele opp datasettet i to: ett til å trene algoritmen og ett annet for å teste den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data() #Denne returnerer to tupler med to lister i hver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prøv å få et forhold til hvordan dataene ser ut. Her er to vektorer hvor hvert element er en matrise\n",
    "print(\"Training data shape: \", x_train.shape)\n",
    "print(\"Test data shape\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hvordan set dataene ut? Legg merke til merkelappene som tilhører hvert bilde.\n",
    "fig = plt.figure()\n",
    "n = 9\n",
    "for i in range(n):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(x_train[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"{}\".format(y_train[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vi husker nå at et _dense_-nettverk (MLP) ikke liker to-dimensjonale input-data. Vi må derfor *kjevle ut* bildene til de blir en én-dimensjonal vektor. \n",
    "\n",
    "+-----+-----+-----+-----+\n",
    "| x11 | x12 | x13 | x14 |\n",
    "+-----+-----+-----+-----+\n",
    "| x21 | x22 | x23 | x24 |\n",
    "+-----+-----+-----+-----+\n",
    "| x31 | x32 | x33 | x34 |\n",
    "+-----+-----+-----+-----+\n",
    "| x41 | x42 | x43 | x44 |\n",
    "+-----+-----+-----+-----+ \n",
    "\n",
    "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+\n",
    "| x11 | x12 | x13 | x14 | x21 | x22 | x23 | x24 | x31 | x32 | x33 | x34 | x41 | x42 | x43 | x44 |\n",
    "+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vector_size = 28*28\n",
    "X_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
    "X_test = x_test.reshape(x_test.shape[0], image_vector_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når det gjelder output, så vil nettverket gi oss sannsynligheten for at bildet er et spesifikt tall. Vi har ti ou-nevroner, hvor den første representerer 0, den andre 1 osv. For nettverket gir selve grunntallene ingen mening. Vi kan forvente at output blir kodet som _one-hot_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10 #Hvor mange klasser har vi (i.e. tall som skal klassifiseres)\n",
    "\n",
    "Y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "Y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training lables 0 - %d as one-hot encoded vectors:\\n\" % n, Y_train[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nå skal vi endelig lage modellen (nettverket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = image_vector_size #Hvor mange piksler skal inn av gangen? Hele bildet?\n",
    "\n",
    "model = Sequential() #Vi lager en sekvensiell modell\n",
    "# Vi legger til ett \"dense-lag\". Input_shape forteller hvordan form input tar.\n",
    "# Activation: hvilken funksjon skal vi ha? Hvilken parameter angir antall noder i \"hidden layer\"?\n",
    "model.add(Dense(units=16, activation='sigmoid', input_shape=(input_size,)))\n",
    "# Med Keras er det ikke nødvendig å angi input-shape for lagene n > 1\n",
    "model.add(Dense(units=num_classes, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hvor mange parametre blir dette totalt? Blyant og papir!\n",
    "model.summary()\n",
    "# Hvor mange fikk du? Hvorfor bommet du?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modellen må kompileres (i.e. settes ihop med loss-funksjon, optimiseringsalgoritme og annet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vi bruker stochastic gradient descent, cross entrophy for å \"måle\" informasjonslikheten mellom y og yhat\n",
    "model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Vi kjører nå fit-funksjonen. Denne tilpasser nettverket ditt til distribusjonen i trenings-settet.\n",
    "# Batch-size: Hvor mange samples skal vi kjøre før vi oppgraderer gradienten? Når har vi behov for å justere \n",
    "# dette hyper-parameteret? Hint: (e.g.) NVidia GTX 1080 Ti Turbo med 11 G ram!\n",
    "# Den er raskere enn \"vanlig\" og presisjonen er ikke så langt\n",
    "# borte fra gd. Når er det en fordel med SGD? Finn.no og Amazon? <- Når de skal anbefale deg noe, har de da tid og\n",
    "# ressurser til å justere modellen med all data de har? (Kanskje Amazon, men ihvertfall ikke Finn).\n",
    "# Kjør gjerne med verbose = True.\n",
    "# Validation: validation_split bestemmer hvor mye av dataen som skal brukes til validering. (Se nedenfor)\n",
    "# Hva returnerer .fit? Den returnerer et history-objekt. Hva inneholder dette? prøv dir(history)\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=5, verbose=False, validation_split=.1)\n",
    "# Loss og accuracy: hva er egentlig dette? Metrikker for å evaluere modellen mot et testsett?\n",
    "loss, accuracy = model.evaluate(X_test, Y_test, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graph(history, loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVARSEL! Bruk denne kun dersom du skal nullstille grafen. Alle parametre blir slettet! \n",
    "#init_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# På tide å se med egne øyne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_eval(54) #Sett inn tall som fungerer for indeks inn i test-settet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# På tide å justere nettverkets hyperparametre!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Du skjønner sikkert at 82 prosent nøyaktighet ikke er bra nok. Hvis du har 1 000 000 siffer, skal du da ha 180 000 feil-klassifiseringer? Se hva du kan gjøre for å øke nøyaktigheten.\n",
    "\n",
    "Modellen:\n",
    "* antall lag\n",
    "* antall noder (eller aktiveringer, som det også kalles)\n",
    "* hvilken aktiveringsfunksjon er best? Sjekk ut:\n",
    "    * ReLu LeakyReLu\n",
    "    * Tangens Hyperbolicus\n",
    "    * SoftMax\n",
    "* Spesielt interesserte kan også sjekke ut\n",
    "    * PReLu\n",
    "    * ArcTan\n",
    "    * SoftPlus\n",
    "    \n",
    "Hvorfor er det ingen god ide å bruke en lineær aktiveringsfunksjon (aka. identitetsfunksjonen)?\n",
    "\n",
    "#### Så til trening av modellen\n",
    "Hva slags optimiseringsalgoritme skal du bruke?\n",
    "* SGD\n",
    "* RMSProp\n",
    "* RMSMomentum\n",
    "* AdaProp\n",
    "* AdaDelta\n",
    "* AdaGrad\n",
    "* Adam\n",
    "* Nadam\n",
    "* Nesterov\n",
    "\n",
    "Hvor mange epochs? \n",
    "\n",
    "Hvordan vil du initialisere vektene? Dette har stor betydning! Du kan også lese om å bryte symmetrien, som er fullstendig avgjørende for et bra resultat. Prøv dir(keras.initializers) eller keras.initializers. <- (trykk tab)\n",
    "\n",
    "\n",
    "$model.add(Dense(64,\n",
    "                kernel\\_initializer='random\\_uniform',\n",
    "                bias\\_initializer='zeros'))$\n",
    "                \n",
    "Les om modellen i Keras-dokumentasjonen, som forøvrig er fremragende!\n",
    "\n",
    "https://keras.io/models/model/\n",
    "\n",
    "Gå tilbake, reset grafen, endre hyper-parametrene og se om du klarer å forbedre resultatet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nevrale nettverk har en del utfordringer (som de fleste andre algoritmer). \n",
    "\n",
    "Hva tror du skjer med nøykaktigheten dersom en sample fra trenings-settet klarer å snike seg inn i test-settet? Hvis vi hadde testet med 10 prosent av trenings-settet?\n",
    "\n",
    "Hva om vi trener for mye på ett trenings-sett? Har noen sett en lineær regresjon med polynomiske variable (som egentlig bare er et spesialtilfelle av lineær regressjon) som har meget store parametre? Hva skjer med training error? Hvordan generaliserer den? Vi får \"høy bias\", dvs. kurven er godt tilpasset dataene være. Dette er ikke ønskelig. På den andre siden, dersom vi ikke trener godt nok eller har for lite trenings-sett, kan vi få altfor høy varianse. Som alt annet i livet, er dette et spørsmål om optimalisering. \n",
    "\n",
    "Bias/variance-tradeoff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
